name: Nightly Combined Report

on:
  # Trigger after both nightly workflows complete
  workflow_run:
    workflows: ["nightly-gtest", "nightly-pytest"]
    types:
      - completed
  # Allow manual trigger
  workflow_dispatch:

permissions:
  contents: read
  actions: read

jobs:
  wait-for-both-workflows:
    runs-on: ubuntu-22.04
    outputs:
      gtest_run_id: ${{ steps.get-runs.outputs.gtest_run_id }}
      pytest_run_id: ${{ steps.get-runs.outputs.pytest_run_id }}
      both_completed: ${{ steps.check-status.outputs.both_completed }}
    steps:
      - name: 'preparation: Harden Runner'
        uses: step-security/harden-runner@6c439dc8bdf85cadbbce9ed30d1c7b959517bc49 # v2.12.2
        with:
          egress-policy: audit

      - name: Wait for both workflows to complete
        id: get-runs
        uses: actions/github-script@v7
        with:
          script: |
            const workflows = ['nightly-gtest', 'nightly-pytest'];
            const runIds = {};
            const statuses = {};

            // Get latest run for each workflow
            for (const workflow of workflows) {
              const runs = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: `${workflow}.yml`,
                per_page: 1,
                status: 'completed'
              });

              if (runs.data.workflow_runs.length > 0) {
                const run = runs.data.workflow_runs[0];
                runIds[workflow.replace('nightly-', '')] = run.id;
                statuses[workflow] = run.conclusion;
                console.log(`${workflow}: Run ID ${run.id}, Status: ${run.conclusion}`);
              } else {
                console.log(`No completed runs found for ${workflow}`);
              }
            }

            core.setOutput('gtest_run_id', runIds['gtest'] || '');
            core.setOutput('pytest_run_id', runIds['pytest'] || '');

            return runIds;

      - name: Check if both workflows completed
        id: check-status
        run: |
          if [ -n "${{ steps.get-runs.outputs.gtest_run_id }}" ] && [ -n "${{ steps.get-runs.outputs.pytest_run_id }}" ]; then
            echo "both_completed=true" >> "$GITHUB_OUTPUT"
            echo "Both workflows have completed runs available"
          else
            echo "both_completed=false" >> "$GITHUB_OUTPUT"
            echo "Waiting for both workflows to complete..."
            exit 1
          fi

  generate-combined-report:
    needs: wait-for-both-workflows
    if: needs.wait-for-both-workflows.outputs.both_completed == 'true'
    runs-on: ubuntu-22.04
    steps:
      - name: 'preparation: Harden Runner'
        uses: step-security/harden-runner@6c439dc8bdf85cadbbce9ed30d1c7b959517bc49 # v2.12.2
        with:
          egress-policy: audit

      - name: 'preparation: Checkout MTL'
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Download pytest artifacts
        uses: actions/download-artifact@f093f21ca4cfa7c75ccbbc2be54da76a0c7e1f05 # v4.4.3
        with:
          pattern: nightly-test-report-*
          path: pytest-reports
          merge-multiple: false
          run-id: ${{ needs.wait-for-both-workflows.outputs.pytest_run_id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download gtest artifacts
        uses: actions/download-artifact@f093f21ca4cfa7c75ccbbc2be54da76a0c7e1f05 # v4.4.3
        with:
          pattern: nightly-gtest-report-*
          path: gtest-reports
          merge-multiple: false
          run-id: ${{ needs.wait-for-both-workflows.outputs.gtest_run_id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Flatten pytest report structure
        run: |
          cd pytest-reports
          for dir in ./nightly-test-report-*; do
            if [ -d "$dir" ] && [ -f "$dir/report.html" ]; then
              mv "$dir/report.html" "${dir}.html"
              rm -rf "$dir"
            fi
          done
          ls -lh ./*.html || echo "No pytest HTML reports found"

      - name: Flatten gtest report structure
        run: |
          cd gtest-reports
          for dir in ./nightly-gtest-report-*; do
            if [ -d "$dir" ] && [ -f "$dir/gtest.log" ]; then
              mv "$dir/gtest.log" "${dir}.log"
              rm -rf "$dir"
            fi
          done
          ls -lh ./*.log || echo "No gtest logs found"

      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          python3 -m pip install pandas beautifulsoup4 openpyxl lxml

      - name: Create combined report script
        run: |
          cat > .github/scripts/combine_all_reports.py << 'EOFPYTHON'
          #!/usr/bin/env python3
          """Combine pytest HTML reports and gtest logs into unified Excel and HTML reports."""

          import argparse
          import re
          import sys
          from datetime import datetime
          from pathlib import Path
          import pandas as pd
          from bs4 import BeautifulSoup
          from openpyxl import Workbook
          from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
          from openpyxl.utils.dataframe import dataframe_to_rows

          def parse_pytest_html(html_file):
              """Parse pytest HTML report and extract test results."""
              with open(html_file, 'r') as f:
                  soup = BeautifulSoup(f.read(), 'html.parser')

              # Extract NIC and category from filename
              # Format: nightly-test-report-{nic}-{category}.html
              filename = Path(html_file).stem
              match = re.match(r'nightly-test-report-([^-]+)-(.+)', filename)
              if match:
                  nic = match.group(1).upper()
                  category = match.group(2)
              else:
                  nic = "UNKNOWN"
                  category = "UNKNOWN"

              # Parse summary statistics
              stats = {'nic': nic, 'category': category}
              for status in ['passed', 'failed', 'skipped', 'error', 'xpassed', 'xfailed']:
                  elem = soup.find('span', class_=status)
                  if elem:
                      count_text = elem.get_text(strip=True)
                      match = re.search(r'(\d+)', count_text)
                      stats[status] = int(match.group(1)) if match else 0
                  else:
                      stats[status] = 0

              stats['total'] = sum(stats.get(k, 0) for k in ['passed', 'failed', 'skipped', 'error', 'xpassed', 'xfailed'])
              return stats

          def parse_gtest_log(log_file):
              """Parse gtest log and extract test results."""
              with open(log_file, 'r') as f:
                  content = f.read()

              # Extract NIC from filename
              # Format: nightly-gtest-report-{nic}.log
              filename = Path(log_file).stem
              match = re.match(r'nightly-gtest-report-(.+)', filename)
              nic = match.group(1).upper() if match else "UNKNOWN"

              results = []

              # Pattern 1: Try to match the summary table rows (preferred format)
              # Format: test_name | passed | failed | skipped | total | pass_rate%
              table_pattern = r'(\S+)\s+\|\s+(\d+)\s+\|\s+(\d+)\s+\|\s+(\d+)\s+\|\s+(\d+)\s+\|\s+([\d.]+)%'
              
              found_table = False
              for line in content.split('\n'):
                  match = re.search(table_pattern, line)
                  if match and match.group(1) not in ['Test', 'TOTAL', '---']:
                      test_category = match.group(1)
                      passed = int(match.group(2))
                      failed = int(match.group(3))
                      skipped = int(match.group(4))
                      total = int(match.group(5))

                      results.append({
                          'nic': nic,
                          'category': test_category,
                          'passed': passed,
                          'failed': failed,
                          'skipped': skipped,
                          'error': 0,
                          'xpassed': 0,
                          'xfailed': 0,
                          'total': total
                      })
                      found_table = True

              # Pattern 2: If no table found, try to extract from gtest individual test results
              # Look for lines like: [ RUN      ] TestSuite.TestName
              #                      [  PASSED  ] TestSuite.TestName (X ms)
              #                      [  FAILED  ] TestSuite.TestName (X ms)
              if not found_table:
                  # Count test results by category
                  test_suites = {}
                  
                  # Find all test runs and their results
                  run_pattern = r'\[\s*RUN\s*\]\s+(\w+)\.(\w+)'
                  passed_pattern = r'\[\s*PASSED\s*\]\s+(\w+)\.(\w+)'
                  failed_pattern = r'\[\s*FAILED\s*\]\s+(\w+)\.(\w+)'
                  skipped_pattern = r'\[\s*SKIPPED\s*\]\s+(\w+)\.(\w+)'
                  
                  for line in content.split('\n'):
                      # Track passed tests
                      match = re.search(passed_pattern, line)
                      if match:
                          suite = match.group(1)
                          if suite not in test_suites:
                              test_suites[suite] = {'passed': 0, 'failed': 0, 'skipped': 0}
                          test_suites[suite]['passed'] += 1
                      
                      # Track failed tests
                      match = re.search(failed_pattern, line)
                      if match:
                          suite = match.group(1)
                          if suite not in test_suites:
                              test_suites[suite] = {'passed': 0, 'failed': 0, 'skipped': 0}
                          test_suites[suite]['failed'] += 1
                      
                      # Track skipped tests
                      match = re.search(skipped_pattern, line)
                      if match:
                          suite = match.group(1)
                          if suite not in test_suites:
                              test_suites[suite] = {'passed': 0, 'failed': 0, 'skipped': 0}
                          test_suites[suite]['skipped'] += 1
                  
                  # Convert to results format
                  for suite, counts in test_suites.items():
                      total = counts['passed'] + counts['failed'] + counts['skipped']
                      if total > 0:
                          results.append({
                              'nic': nic,
                              'category': suite,
                              'passed': counts['passed'],
                              'failed': counts['failed'],
                              'skipped': counts['skipped'],
                              'error': 0,
                              'xpassed': 0,
                              'xfailed': 0,
                              'total': total
                          })

              # Pattern 3: If still no results, try to find summary at the end
              # Format: [  PASSED  ] X tests.
              #         [  FAILED  ] Y tests, listed below:
              if not results:
                  total_passed = 0
                  total_failed = 0
                  
                  for line in content.split('\n'):
                      if re.search(r'\[\s*PASSED\s*\]\s+(\d+)\s+test', line):
                          match = re.search(r'(\d+)', line)
                          if match:
                              total_passed = int(match.group(1))
                      elif re.search(r'\[\s*FAILED\s*\]\s+(\d+)\s+test', line):
                          match = re.search(r'(\d+)', line)
                          if match:
                              total_failed = int(match.group(1))
                  
                  if total_passed > 0 or total_failed > 0:
                      results.append({
                          'nic': nic,
                          'category': 'all_tests',
                          'passed': total_passed,
                          'failed': total_failed,
                          'skipped': 0,
                          'error': 0,
                          'xpassed': 0,
                          'xfailed': 0,
                          'total': total_passed + total_failed
                      })

              return results

          def create_excel_report(pytest_data, gtest_data, output_file):
              """Create Excel report with separate sheets for pytest and gtest."""
              wb = Workbook()

              # Remove default sheet
              wb.remove(wb.active)

              # Create pytest sheet
              if pytest_data:
                  ws_pytest = wb.create_sheet("Pytest Results")
                  df_pytest = pd.DataFrame(pytest_data)
                  df_pytest = df_pytest.sort_values(['nic', 'category'])

                  # Add headers with styling
                  headers = ['NIC', 'Category', 'Passed', 'Failed', 'Skipped', 'Error', 'XPassed', 'XFailed', 'Total']
                  ws_pytest.append(headers)

                  # Style header row
                  header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                  header_font = Font(bold=True, color="FFFFFF")
                  for cell in ws_pytest[1]:
                      cell.fill = header_fill
                      cell.font = header_font
                      cell.alignment = Alignment(horizontal='center')

                  # Add data rows
                  for _, row in df_pytest.iterrows():
                      ws_pytest.append([
                          row['nic'], row['category'], row['passed'], row['failed'],
                          row['skipped'], row['error'], row['xpassed'], row['xfailed'], row['total']
                      ])

                  # Auto-adjust column widths
                  for column in ws_pytest.columns:
                      max_length = max(len(str(cell.value)) for cell in column)
                      ws_pytest.column_dimensions[column[0].column_letter].width = min(max_length + 2, 50)

              # Create gtest sheet
              if gtest_data:
                  ws_gtest = wb.create_sheet("GTest Results")
                  df_gtest = pd.DataFrame(gtest_data)
                  df_gtest = df_gtest.sort_values(['nic', 'category'])

                  headers = ['NIC', 'Test Category', 'Passed', 'Failed', 'Skipped', 'Total']
                  ws_gtest.append(headers)

                  # Style header row
                  header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                  header_font = Font(bold=True, color="FFFFFF")
                  for cell in ws_gtest[1]:
                      cell.fill = header_fill
                      cell.font = header_font
                      cell.alignment = Alignment(horizontal='center')

                  # Add data rows
                  for _, row in df_gtest.iterrows():
                      ws_gtest.append([
                          row['nic'], row['category'], row['passed'], row['failed'],
                          row['skipped'], row['total']
                      ])

                  # Auto-adjust column widths
                  for column in ws_gtest.columns:
                      max_length = max(len(str(cell.value)) for cell in column)
                      ws_gtest.column_dimensions[column[0].column_letter].width = min(max_length + 2, 50)

              # Create summary sheet
              ws_summary = wb.create_sheet("Summary", 0)

              summary_data = []
              summary_data.append(['MTL Nightly Test Report Summary'])
              summary_data.append(['Generated:', datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')])
              summary_data.append([])

              # Calculate totals
              pytest_total = pytest_passed = pytest_failed = pytest_skipped = 0
              gtest_total = gtest_passed = gtest_failed = gtest_skipped = 0

              if pytest_data:
                  df_pytest = pd.DataFrame(pytest_data)
                  pytest_total = df_pytest['total'].sum()
                  pytest_passed = df_pytest['passed'].sum()
                  pytest_failed = df_pytest['failed'].sum()
                  pytest_skipped = df_pytest['skipped'].sum()
                  pytest_pass_rate = (pytest_passed / pytest_total * 100) if pytest_total > 0 else 0
                  
                  summary_data.append(['Pytest Summary'])
                  summary_data.append(['Total Tests:', pytest_total])
                  summary_data.append(['Total Passed:', pytest_passed])
                  summary_data.append(['Total Failed:', pytest_failed])
                  summary_data.append(['Total Skipped:', pytest_skipped])
                  summary_data.append(['Pass Rate:', f'{pytest_pass_rate:.2f}%'])
                  summary_data.append([])

              if gtest_data:
                  df_gtest = pd.DataFrame(gtest_data)
                  gtest_total = df_gtest['total'].sum()
                  gtest_passed = df_gtest['passed'].sum()
                  gtest_failed = df_gtest['failed'].sum()
                  gtest_skipped = df_gtest['skipped'].sum()
                  gtest_pass_rate = (gtest_passed / gtest_total * 100) if gtest_total > 0 else 0
                  
                  summary_data.append(['GTest Summary'])
                  summary_data.append(['Total Tests:', gtest_total])
                  summary_data.append(['Total Passed:', gtest_passed])
                  summary_data.append(['Total Failed:', gtest_failed])
                  summary_data.append(['Total Skipped:', gtest_skipped])
                  summary_data.append(['Pass Rate:', f'{gtest_pass_rate:.2f}%'])
                  summary_data.append([])

              # Overall combined summary
              if pytest_data or gtest_data:
                  combined_total = pytest_total + gtest_total
                  combined_passed = pytest_passed + gtest_passed
                  combined_failed = pytest_failed + gtest_failed
                  combined_skipped = pytest_skipped + gtest_skipped
                  combined_pass_rate = (combined_passed / combined_total * 100) if combined_total > 0 else 0
                  
                  summary_data.append(['Overall Summary (Pytest + GTest)'])
                  summary_data.append(['Total Tests:', combined_total])
                  summary_data.append(['Total Passed:', combined_passed])
                  summary_data.append(['Total Failed:', combined_failed])
                  summary_data.append(['Total Skipped:', combined_skipped])
                  summary_data.append(['Pass Rate:', f'{combined_pass_rate:.2f}%'])

              for row in summary_data:
                  ws_summary.append(row)

              # Style summary sheet
              ws_summary['A1'].font = Font(bold=True, size=14)
              ws_summary.column_dimensions['A'].width = 30
              ws_summary.column_dimensions['B'].width = 20

              wb.save(output_file)
              print(f"Excel report saved to: {output_file}")

          def create_html_report(pytest_data, gtest_data, output_file):
              """Create HTML report combining pytest and gtest results."""
              html_template = """
              <!DOCTYPE html>
              <html>
              <head>
                  <meta charset="utf-8"/>
                  <title>MTL Nightly Test Report</title>
                  <style>
                      body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
                      h1 {{ color: #366092; border-bottom: 2px solid #366092; padding-bottom: 10px; }}
                      h2 {{ color: #555; margin-top: 30px; }}
                      .summary {{ background: white; padding: 20px; border-radius: 5px; margin: 20px 0; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
                      .summary-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; }}
                      .summary-card {{ padding: 15px; border-radius: 5px; text-align: center; }}
                      .summary-card h3 {{ margin: 0; font-size: 14px; color: #666; }}
                      .summary-card .value {{ font-size: 32px; font-weight: bold; margin: 10px 0; }}
                      .passed {{ background-color: #d4edda; color: #155724; }}
                      .failed {{ background-color: #f8d7da; color: #721c24; }}
                      .skipped {{ background-color: #fff3cd; color: #856404; }}
                      table {{ border-collapse: collapse; width: 100%; background: white; margin: 20px 0; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
                      th {{ background-color: #366092; color: white; padding: 12px; text-align: left; }}
                      td {{ padding: 10px; border-bottom: 1px solid #ddd; }}
                      tr:hover {{ background-color: #f5f5f5; }}
                      .timestamp {{ color: #666; font-size: 0.9em; margin-bottom: 20px; }}
                  </style>
              </head>
              <body>
                  <h1>MTL Nightly Test Report - Combined Results</h1>
                  <div class="timestamp">Generated: {timestamp}</div>

                  {overall_summary}
                  {pytest_summary}
                  {gtest_summary}

                  <h2>Pytest Results by NIC and Category</h2>
                  {pytest_table}

                  <h2>GTest Results by NIC and Test Category</h2>
                  {gtest_table}
              </body>
              </html>
              """

              timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')

              # Calculate overall totals
              pytest_total = pytest_passed = pytest_failed = pytest_skipped = 0
              gtest_total = gtest_passed = gtest_failed = gtest_skipped = 0
              
              if pytest_data:
                  df_pytest = pd.DataFrame(pytest_data)
                  pytest_total = df_pytest['total'].sum()
                  pytest_passed = df_pytest['passed'].sum()
                  pytest_failed = df_pytest['failed'].sum()
                  pytest_skipped = df_pytest['skipped'].sum()
              
              if gtest_data:
                  df_gtest = pd.DataFrame(gtest_data)
                  gtest_total = df_gtest['total'].sum()
                  gtest_passed = df_gtest['passed'].sum()
                  gtest_failed = df_gtest['failed'].sum()
                  gtest_skipped = df_gtest['skipped'].sum()
              
              # Calculate combined totals
              combined_total = pytest_total + gtest_total
              combined_passed = pytest_passed + gtest_passed
              combined_failed = pytest_failed + gtest_failed
              combined_skipped = pytest_skipped + gtest_skipped
              
              # Calculate pass rates
              pytest_pass_rate = (pytest_passed / pytest_total * 100) if pytest_total > 0 else 0
              gtest_pass_rate = (gtest_passed / gtest_total * 100) if gtest_total > 0 else 0
              combined_pass_rate = (combined_passed / combined_total * 100) if combined_total > 0 else 0

              # Generate overall summary
              overall_summary = f"""
              <div class="summary">
                  <h2>Overall Summary</h2>
                  <div class="summary-grid">
                      <div class="summary-card">
                          <h3>Total Tests</h3>
                          <div class="value">{combined_total}</div>
                      </div>
                      <div class="summary-card passed">
                          <h3>Passed</h3>
                          <div class="value">{combined_passed}</div>
                      </div>
                      <div class="summary-card failed">
                          <h3>Failed</h3>
                          <div class="value">{combined_failed}</div>
                      </div>
                      <div class="summary-card skipped">
                          <h3>Skipped</h3>
                          <div class="value">{combined_skipped}</div>
                      </div>
                      <div class="summary-card passed">
                          <h3>Pass Rate</h3>
                          <div class="value">{combined_pass_rate:.2f}%</div>
                      </div>
                  </div>
              </div>
              """

              # Generate pytest summary
              pytest_summary = ""
              if pytest_data:
                  pytest_summary = f"""
                  <div class="summary">
                      <h2>Pytest Summary</h2>
                      <div class="summary-grid">
                          <div class="summary-card">
                              <h3>Total Tests</h3>
                              <div class="value">{pytest_total}</div>
                          </div>
                          <div class="summary-card passed">
                              <h3>Passed</h3>
                              <div class="value">{pytest_passed}</div>
                          </div>
                          <div class="summary-card failed">
                              <h3>Failed</h3>
                              <div class="value">{pytest_failed}</div>
                          </div>
                          <div class="summary-card skipped">
                              <h3>Skipped</h3>
                              <div class="value">{pytest_skipped}</div>
                          </div>
                          <div class="summary-card passed">
                              <h3>Pass Rate</h3>
                              <div class="value">{pytest_pass_rate:.2f}%</div>
                          </div>
                      </div>
                  </div>
                  """

              # Generate gtest summary
              gtest_summary = ""
              if gtest_data:
                  gtest_summary = f"""
                  <div class="summary">
                      <h2>GTest Summary</h2>
                      <div class="summary-grid">
                          <div class="summary-card">
                              <h3>Total Tests</h3>
                              <div class="value">{gtest_total}</div>
                          </div>
                          <div class="summary-card passed">
                              <h3>Passed</h3>
                              <div class="value">{gtest_passed}</div>
                          </div>
                          <div class="summary-card failed">
                              <h3>Failed</h3>
                              <div class="value">{gtest_failed}</div>
                          </div>
                          <div class="summary-card skipped">
                              <h3>Skipped</h3>
                              <div class="value">{gtest_skipped}</div>
                          </div>
                          <div class="summary-card passed">
                              <h3>Pass Rate</h3>
                              <div class="value">{gtest_pass_rate:.2f}%</div>
                          </div>
                      </div>
                  </div>
                  """

              # Generate pytest table
              pytest_table = ""
              if pytest_data:
                  df_pytest = pd.DataFrame(pytest_data)
                  pytest_table = df_pytest.to_html(index=False, classes='data-table')

              # Generate gtest table
              gtest_table = ""
              if gtest_data:
                  df_gtest = pd.DataFrame(gtest_data)
                  gtest_table = df_gtest[['nic', 'category', 'passed', 'failed', 'skipped', 'total']].to_html(index=False, classes='data-table')

              html = html_template.format(
                  timestamp=timestamp,
                  overall_summary=overall_summary,
                  pytest_summary=pytest_summary,
                  gtest_summary=gtest_summary,
                  pytest_table=pytest_table,
                  gtest_table=gtest_table
              )

              with open(output_file, 'w') as f:
                  f.write(html)

              print(f"HTML report saved to: {output_file}")

          def main():
              parser = argparse.ArgumentParser(description='Combine pytest and gtest nightly reports')
              parser.add_argument('--pytest-dir', type=Path, required=True, help='Directory containing pytest HTML reports')
              parser.add_argument('--gtest-dir', type=Path, required=True, help='Directory containing gtest log files')
              parser.add_argument('--output-excel', type=Path, default='combined_nightly_report.xlsx', help='Output Excel file')
              parser.add_argument('--output-html', type=Path, default='combined_nightly_report.html', help='Output HTML file')

              args = parser.parse_args()

              # Parse pytest reports
              pytest_data = []
              if args.pytest_dir.exists():
                  for html_file in args.pytest_dir.glob('*.html'):
                      try:
                          stats = parse_pytest_html(html_file)
                          pytest_data.append(stats)
                          print(f"Parsed pytest report: {html_file.name}")
                      except Exception as e:
                          print(f"Error parsing {html_file}: {e}", file=sys.stderr)

              # Parse gtest logs
              gtest_data = []
              if args.gtest_dir.exists():
                  for log_file in args.gtest_dir.glob('*.log'):
                      try:
                          results = parse_gtest_log(log_file)
                          gtest_data.extend(results)
                          print(f"Parsed gtest log: {log_file.name}")
                      except Exception as e:
                          print(f"Error parsing {log_file}: {e}", file=sys.stderr)

              if not pytest_data and not gtest_data:
                  print("No test data found!", file=sys.stderr)
                  sys.exit(1)

              # Create reports
              create_excel_report(pytest_data, gtest_data, args.output_excel)
              create_html_report(pytest_data, gtest_data, args.output_html)

              print(f"\nSummary:")
              print(f"  Pytest reports processed: {len(pytest_data)}")
              print(f"  GTest categories processed: {len(gtest_data)}")

          if __name__ == '__main__':
              main()
          EOFPYTHON

          chmod +x .github/scripts/combine_all_reports.py

      - name: Generate combined reports
        id: combine
        run: |
          python3 .github/scripts/combine_all_reports.py \
            --pytest-dir pytest-reports \
            --gtest-dir gtest-reports \
            --output-excel combined_nightly_report.xlsx \
            --output-html combined_nightly_report.html

          if [ -f "combined_nightly_report.xlsx" ] && [ -f "combined_nightly_report.html" ]; then
            echo "reports_generated=true" >> "$GITHUB_OUTPUT"
          else
            echo "reports_generated=false" >> "$GITHUB_OUTPUT"
            exit 1
          fi

      - name: Upload combined Excel report
        if: steps.combine.outputs.reports_generated == 'true'
        uses: actions/upload-artifact@b4b15b8c7c6ac21ea08fcf65892d2ee8f75cf882 # v4.4.3
        with:
          name: nightly-combined-report-excel
          path: combined_nightly_report.xlsx

      - name: Upload combined HTML report
        if: steps.combine.outputs.reports_generated == 'true'
        uses: actions/upload-artifact@b4b15b8c7c6ac21ea08fcf65892d2ee8f75cf882 # v4.4.3
        with:
          name: nightly-combined-report-html
          path: combined_nightly_report.html